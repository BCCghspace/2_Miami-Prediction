---
title: "Chapter 3 Demo"
author: "Anna Duan"
date: "9/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load libraries, etc
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(mapview)

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

#this function converts a column in to quintiles. It is used for mapping.
qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                          c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}
```

## Read Boston Data
```{r data}
#read geojson, project
nhoods <- 
  st_read("http://bostonopendata-boston.opendata.arcgis.com/datasets/3525b0ee6e6b427f9aab5d0a1d0a1a28_0.geojson") %>%
  st_transform('ESRI:102286')
#read csv
boston <- 
  read.csv(paste0(root.dir,"/Chapter3_4/bostonHousePriceData_clean.csv"))

boston.sf <-  #make csv file an sf, project it
  boston %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102286')

```

## Map sales prices

You can also embed plots, for example:

```{r salesPrices, echo=FALSE}
ggplot() +
  geom_sf(data = nhoods, fill = "black") +
  geom_sf(data = boston.sf, aes(colour = q5(PricePerSq)), 
          show.legend = "point", size = 0.7) +
  scale_colour_manual(values = palette5,
                   labels=qBr(boston,"PricePerSq"),
                   name="Quintile\nBreaks") +
  labs(title="Price Per Square Foot, Boston") +
  mapTheme()
```
## Read crime
```{r crime}
#read crime data
bostonCrimes <- read.csv("/Users/annaduan/Documents/GitHub/Public-Policy-Analytics-Landing/DATA/Chapter3_4/bostonCrimes.csv")

group_by(bostonCrimes, OFFENSE_CODE_GROUP) %>%
  summarize(count = n()) %>%
  arrange(-count) %>% top_n(5)

#subset, map aggravated assault incidents 
bostonCrimes.sf <-  #make bostonCrimes(csv) into simple feature object
  bostonCrimes %>%  
    filter(OFFENSE_CODE_GROUP == "Aggravated Assault",    #filter for aggravated assault
           Lat > -1) %>%           #exclude lattitudes -1 and under (I guess?)
    dplyr::select(Lat, Long) %>%   #select lat,long columns
    na.omit() %>%                  #delete rows with missing values
    st_as_sf(coords = c("Long", "Lat"), crs = 4326, agr = "constant") %>%    #turn into sf, re-project to ESRI
    st_transform('ESRI:102286') %>%
    distinct()                    #keep only unique rows

ggplot() + geom_sf(data = nhoods, fill = "grey40") +          #map neighborhoods, gray fill
  stat_density2d(data = data.frame(st_coordinates(bostonCrimes.sf)),    #2d density estimate, draw contour lines
                 aes(X, Y, fill = ..level.., alpha = ..level..),        #specs for above^
                 size = 0.01, bins = 40, geom = 'polygon') +             #^^
   scale_fill_gradient(low = "#25CB10", high = "#FA7800", name = "Density") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Aggravated Assaults, Boston") +
  mapTheme()
```

}}
## How to parameterize/model "how much crime"
1. sum incidents by (arbitrary) areal unit - e.g. census tract
  least ideal (MAUP) 

2. sum incidents within specified buffer from each house sale
  still not ideal - assumes consistent relationship between crime & sales prices citywide
```{r crimBuffer, echo=FALSE}
  boston.sf$crimes.Buffer =          #add crimes/buffer for each home sale
    st_buffer(boston.sf, 660) %>%    #create buffer from home sales locations sf
    aggregate(mutate(bostonCrimes.sf, counter = 1),., sum) %>%   #spatial join sales buffer w crimes
    pull(counter)              #pull counter column -> change from sf (coords) layer to numbers
```
  
3. avg nearest neighbor distance (from home to its k nearest neighbors)
  still a flawed approach but captures continuous variations in short distances best
```{r nearestNeighbor}
 #load the function manually
nn_function <- function(measureFrom,measureTo,k) {  
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist #make matrix of distances from measureFroms to k measureTos
  
    output <-
      as.data.frame(nn) %>%    #make into data frame
      rownames_to_column(var = "thisPoint") %>%    #make field for each measureFrom point
      gather(points, point_distance, V1:ncol(.)) %>% #wide -> long form
      arrange(as.numeric(thisPoint)) %>%            #sort measurefrom field (asc.)
      group_by(thisPoint) %>%                       #group distances by measureFrom points
      summarize(pointDistance = mean(point_distance)) %>% #get means
      arrange(as.numeric(thisPoint)) %>%     #make thisPoint numeric, sort     
      dplyr::select(-thisPoint) %>%          #get rid of thisPoint
      pull()                          #pull avg nn distance
  
  return(output)  
    
    #create features of 1, 2, 3, 4, and 5 nearest neighbors
st_c <- st_coordinates        #convert data frame to xy coordinate matrix

boston.sf <-                  
  boston.sf %>% 
    mutate(         #mutate new variables, from boston.sf to k bostonCrimes spots
      crime_nn1 = nn_function(st_c(boston.sf), st_c(bostonCrimes.sf), 1),
      crime_nn2 = nn_function(st_c(boston.sf), st_c(bostonCrimes.sf), 2), 
      crime_nn3 = nn_function(st_c(boston.sf), st_c(bostonCrimes.sf), 3), 
      crime_nn4 = nn_function(st_c(boston.sf), st_c(bostonCrimes.sf), 4), 
      crime_nn5 = nn_function(st_c(boston.sf), st_c(bostonCrimes.sf), 5)) 
}
```

## Exploratory analysis: correlation
```{r correlation}
st_drop_geometry(boston.sf) %>%       #drop geometry from boston home sales data
  mutate(Age = 2015 - YR_BUILT) %>%   #new variable: age
  dplyr::select(SalePrice, LivingArea, Age, GROSS_AREA) %>%  #subset 4 variables
  filter(SalePrice <= 1000000, Age < 500) %>%                #filter for rpice, age
  gather(Variable, Value, -SalePrice) %>%                  #make into long form
   ggplot(aes(Value, SalePrice)) +                        #plot by value, sales price
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()

boston %>% 
  dplyr::select(SalePrice, Style, OWN_OCC, NUM_FLOORS) %>% #select these variables as subset
  mutate(NUM_FLOORS = as.factor(NUM_FLOORS)) %>%    #make NUM_FLOORS into a factor. This means make it a category variabe
  filter(SalePrice <= 1000000) %>%         #filter for houses less than/equal to that price
  gather(Variable, Value, -SalePrice) %>%  #make into long form data
   ggplot(aes(Value, SalePrice)) +         #plot style, ownerOcc, numFloors by sales price 
     geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +    
     facet_wrap(~Variable, ncol = 1, scales = "free") +       #mini plot for each variable, in 1 column (I think?)
     labs(title = "Price as a function of categorical variables", y = "Mean_Price") +
     theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
     plotTheme()

#Correlation matrix plot
numericVars <- 
  select_if(st_drop_geometry(boston.sf), is.numeric) %>% na.omit() 

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 

#r correlation test
cor.test(boston$LivingArea, boston$SalePrice, method = "pearson")

#OLS regression model
livingReg <- lm(SalePrice ~ LivingArea, data = boston)  #sales price: function of living area
summary(livingReg)  #show results of regression
  #constant: sales price if area was 0
  #coefficient for living area: amount that sales price changes for each ft^2 âˆ†
  #R^2: tells us % of dependent (price) that's explained by relationship w independent (area). It's linear so R^2 of 0.8 explains 2x more than 0.4
  #if only small part of independent is explained by coefficient, error will be high, need to add more features
  #lm doesn't take sf layers, need to drop geometries
reg1 <- lm(SalePrice ~ ., data = st_drop_geometry(boston.sf) %>% 
                                 dplyr::select(SalePrice, LivingArea, Style, 
                                               GROSS_AREA, R_TOTAL_RM, NUM_FLOORS,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE))
summary(reg1)
  #now they explain 56%

```

```{r feature eng}
#What if we make number of floors into categorical, not continuous variable?
boston.sf <- 
  boston.sf %>%
  mutate(NUM_FLOORS.cat = case_when(
                  NUM_FLOORS >= 0 & NUM_FLOORS < 3  ~ "Up to 2 Floors",
                  NUM_FLOORS >= 3 & NUM_FLOORS < 4  ~ "3 Floors",
                  NUM_FLOORS > 4                    ~ "4+ Floors"))

#reg 2 shows us what happens: now, the feature has more predictive power (peep r^2)
reg2 <- lm(SalePrice ~ ., data = st_drop_geometry(boston.sf) %>% 
                                 dplyr::select(SalePrice, LivingArea, Style, 
                                               GROSS_AREA, R_TOTAL_RM, NUM_FLOORS.cat,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, R_KITCH, 
                                               R_AC, R_FPLACE))
summary(reg2)
```


```{r geojson}
#this was just to peep studentsData
trainingData <- st_read("/Users/annaduan/Downloads/studentsData.geojson")
```

## More feature engineering + colinearity
### Accuracy: mean average error
```{r accuracy}
inTrain <- createDataPartition(     #randomly split into test/training dataset
              y = paste(boston.sf$NUM_FLOORS.cat, boston.sf$Style, boston.sf$R_AC),  #R_AC: 3 categories
              p = .60, list = FALSE)
boston.training <- boston.sf[inTrain,] 
boston.test <- boston.sf[-inTrain,]  

reg.training <- lm(SalePrice ~ ., data = st_drop_geometry(boston.training) %>% 
                                    dplyr::select(SalePrice, LivingArea, Style, 
                                               GROSS_AREA, NUM_FLOORS.cat,
                                               R_BDRMS, R_FULL_BTH, R_HALF_BTH, 
                                               R_KITCH, R_AC, R_FPLACE,
                                               crimes.Buffer))
summary(reg.training)
  #we see above that crime reduces sales price by more than $4000
  #how can we know how well the model predicts unknown prices?
  #following, we add 3 variables to boston.test: 
    #1. sales price error
    #2. sales price absolute error
    #3. sales price absolute percent error
    #compared with R^2, better validation approach
boston.test <-
  boston.test %>%
  mutate(SalePrice.Predict = predict(reg.training, boston.test),  #make new variables
         SalePrice.Error = SalePrice.Predict - SalePrice,         
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)%>%
  filter(SalePrice < 5000000)     #filter out 5million+ sales prices
#lets see how it looks:
mean(boston.test$SalePrice.AbsError, na.rm = T)
# mean error is $175,686.7 - looking rough... (mean sales price is $628,338)
mean(boston.test$SalePrice.APE, na.rm = T)
# not greater either^ - 30% 
# can plot observed x predicted price to see where more feature are needed (eg lower or higher home sales prices)



#generalizability
  # use cross validation to check - k-Fold algorithm
    # use k "folds"/subsets, train each on a subset of observations, predict on test set,measuree goodness of fit
    # average goodness of fit across all k folds
    # if it's generalizable, we should see similar goodness of fit metrics across each fold 
fitControl <- trainControl(method = "cv", number = 100)    #fitControl specifies number of folds (100)
set.seed(825)  #set seed ensures reproducible folds

reg.cv <-                                                       #get regression
  train(SalePrice ~ ., data = st_drop_geometry(boston.sf) %>% 
                                dplyr::select(SalePrice, 
                                LivingArea, Style, GROSS_AREA, 
                                NUM_FLOORS.cat, R_BDRMS, R_FULL_BTH, 
                                R_HALF_BTH, R_KITCH, R_AC, 
                                R_FPLACE, crimes.Buffer), 
     method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv


##spatial distribution of errors matters! This is why we look at spatial processes next...
```

